---
title: "P8105_HW2_msp2210"
output: github_document
author: "Mukta Patwari"
date: "2025-09-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set-up
```{r libraries}
library(tidyverse)
library(haven)
library(readxl)
```

## Problem 1

Cleaning 'pols-month' dataset
```{r pols-month}
pols_month_df =
  read_csv("fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names () %>% 
  separate(
    mon, into = c("year", "month", "day"),
    sep = "-"
  ) %>% 
  mutate(
    month = month.name[as.numeric(month)]
  ) %>% 
  mutate(
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) %>% 
  select(-prez_dem, -prez_gop, -day)
```

Cleaning 'snp' dataset
```{r snp}
snp_df =
  read_csv("fivethirtyeight_datasets/snp.csv", 
      col_types = cols(date = col_date(format = "%m/%d/%y"))) %>% 
  janitor::clean_names() %>% 
  separate(
    date, into = c("year", "month", "day"),
    convert = TRUE,
  ) %>% 
  mutate(
    month = month.name[as.numeric(month)],
    year = as.character(year)
  ) %>% 
  select (-day)
```

Cleaning 'unemployment' dataset
```{r unemployment}
unemployment_df =
  read_csv("fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = jan:dec,
    names_to = "month",
    values_to = "percent_unemployed"
  ) %>% 
  mutate(
    month = str_to_title(month),
    year = as.character(year)
  )
```

Merging the 'pols-month' and 'snp' datasets together
```{r first-merge}
merged_pols_snp =
  left_join(pols_month_df, snp_df, by = c("year", "month"))
```

Merging the new dataset with 'unemployment'
```{r second-merge}
merged_pols_snp_unemployment =
  left_join(merged_pols_snp, unemployment_df, by = c("year", "month"))
```

The cleaned 'pols-month dataset' contains `r nrow(pols_month_df)` rows and the following `r ncol(pols_month_df)` variables: `r colnames(pols_month_df)`. This dataset relates to politicians in the U.S., like what position they hold (presidential or in Congress), and their party affiliation, Democrat or Republican. The 'snp' dataset refers to the S&P index, and contains `r nrow(snp_df)` rows with the date of the observation and the price at closing. The cleaned 'unemployment' dataset contains `r nrow(unemployment_df)` rows and gives the percentage of people unemployed in the month of each year. 

Combining these three data sets creates a merged dataset of `r nrow(merged_pols_snp_unemployment)` rows with these variables: `r colnames(merged_pols_snp_unemployment)`. The years range from `r min(pull(merged_pols_snp_unemployment, year))` to `r max(pull(merged_pols_snp_unemployment, year))`, which helps to capture the differences in political affiliations, stock market events, and unemployment rates. As the initial 'snp' dataset and 'unemployment' dataset contained less than 822 observations, there are unfortunately several NAs for the columns of close and percent_unemployed in the merged dataset. 

## Problem 2

Cleaning Mr. Trash Wheel dataset
```{r mrtrashwheel}
mrtrash_df =
  read_excel("202509 Trash Wheel Collection Data.xlsx", sheet = 1, range = "A2:N709", col_names = TRUE, na = c("NA", ".", "")) %>%
  drop_na(Dumpster) %>% 
  janitor::clean_names() %>%
  mutate(
    year = as.numeric(year)
  ) %>% 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  ) %>% 
  mutate(
    trashwheel_name = "Mr. Trash Wheel"
  )
```

Cleaning Professor Trash Wheel dataset
```{r proftrash}
proftrash_df =
  read_excel("202509 Trash Wheel Collection Data.xlsx", sheet = 2, range = "A2:M134", col_names = TRUE, na = c("NA", ".", "")) %>% 
  drop_na(Dumpster) %>% 
  janitor::clean_names() %>%
  mutate(
    trashwheel_name = "Professor Trash Wheel"
  )
```

Cleaning Gwynnda dataset
```{r gwynnda}
gwynnda_df =
  read_excel("202509 Trash Wheel Collection Data.xlsx", sheet = 4, range = "A2:L351", col_names = TRUE, na = c("NA", ".", "")) %>% 
  drop_na(Dumpster) %>% 
  janitor::clean_names() %>%
  mutate(
    trashwheel_name = "Gwynnda Trash Wheel"
  )
```

Merging datasets
```{r trash-wheel-merge}
trashwheel_df =
  bind_rows(mrtrash_df, proftrash_df, gwynnda_df)
```

The cleaned dataset contains `r nrow(trashwheel_df)` observations of trash collected by Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. Key variables include `dumpster` (the dumpster number where the trash is collected) `weight_tons` (the weight in tons of trash collected), `trashwheel_name` (shows which trash wheel collected the trash). An interesting variable is `homes_powered`, which captures how many homes benefitted from the trash wheel collecting trash to be incinerated and to be converted to electricity. The total weight in tons of trash collected by Professor Trash Wheel is `r trashwheel_df %>% filter(trashwheel_name == "Professor Trash Wheel") %>% pull(weight_tons) %>% sum()` tons. The total number of cigarette butts collected by Gwynnda in June of 2022 was `r trashwheel_df %>% filter(trashwheel_name == "Gwynnda Trash Wheel", year == 2022, month == "June") %>% pull(cigarette_butts) %>% sum() %>% format(scientific = FALSE)`.

## Problem 3

Importing and tidying zipcodes dataset
```{r zipcodes}
zipcodes_df =
  read_csv("zillow_data/Zip Codes.csv") %>%
  drop_na() %>% 
  janitor::clean_names() %>% 
  mutate(file_date = as.Date(file_date, format = "%m/%d/%y")) %>% 
  rename(date = file_date)
```

Importing and tidying ZORI data
```{r zori}
zori_df =
  read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") %>% 
  drop_na() %>% 
  rename_with(janitor::make_clean_names, .cols = 1:9) %>% 
  pivot_longer(
    cols = "2015-01-31":"2024-08-31",
    names_to = "date",
    values_to = "rent",
  ) %>% 
  mutate(
    county_name = str_trim(gsub("County", "", county_name)),
    date = as.Date(date)
  ) %>% 
  rename(zip_code = region_name, county = county_name)
```

Merging zip codes and ZORI datasets
```{r zillow}
zillow_df =
  full_join(
    zori_df, zipcodes_df
    ) %>%
  relocate(date, zip_code, county)
```

Finding unique observations
```{r}
zillow_df %>% 
  select(zip_code) %>% 
  distinct()
```


The new merged dataset has `r nrow(zillow_df)` observations. There are 179 unique zip codes in the dataset...